{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VVimp-week3_task2_fine_tuning_clean.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehaRima/2sxc-content-app/blob/master/VVimp_week3_task2_fine_tuning_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F33GlO9vR_6k"
      },
      "source": [
        "# Fine-tuning InceptionV3 for flowers classification\n",
        "\n",
        "In this task you will fine-tune InceptionV3 architecture for flowers classification task.\n",
        "\n",
        "InceptionV3 architecture (https://research.googleblog.com/2016/03/train-your-own-image-classifier-with.html):\n",
        "<img src=\"https://github.com/hse-aml/intro-to-dl/blob/master/week3/images/inceptionv3.png?raw=1\" style=\"width:70%\">\n",
        "\n",
        "Flowers classification dataset (http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html) consists of 102 flower categories commonly occurring in the United Kingdom. Each class contains between 40 and 258 images:\n",
        "<img src=\"https://github.com/hse-aml/intro-to-dl/blob/master/week3/images/flowers.jpg?raw=1\" style=\"width:70%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0H9Kf_qsR_6s"
      },
      "source": [
        "# Import stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyw73pdkU5_C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7db01207-ed76-4a76-9044-cfbb892b37fd"
      },
      "source": [
        "!pip install matplotlib-venn\n",
        "!apt-get -qq install -y libfluidsynth1\n",
        "# To determine which version you're using:\n",
        "!pip show tensorflow\n",
        "\n",
        "# For the current version: \n",
        "!pip install --upgrade tensorflow\n",
        "\n",
        "# For a specific version:\n",
        "!pip install tensorflow==1.2\n",
        "!pip install keras==2.0.6\n",
        "\n",
        "# For the latest nightly build:\n",
        "!pip install tf-nightly\n",
        "# https://pypi.python.org/pypi/libarchive\n",
        "!apt-get -qq install -y libarchive-dev && pip install -U libarchive\n",
        "import libarchive\n",
        "!apt-get -qq install python-cartopy python3-cartopy\n",
        "import cartopy\n",
        "# https://pypi.python.org/pypi/pydot\n",
        "!apt-get -qq install -y graphviz && pip install pydot\n",
        "import pydot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.6/dist-packages (0.11.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from matplotlib-venn) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from matplotlib-venn) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from matplotlib-venn) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->matplotlib-venn) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->matplotlib-venn) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->matplotlib-venn) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->matplotlib-venn) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->matplotlib-venn) (1.15.0)\n",
            "Selecting previously unselected package libfluidsynth1:amd64.\n",
            "(Reading database ... 144579 files and directories currently installed.)\n",
            "Preparing to unpack .../libfluidsynth1_1.1.9-1_amd64.deb ...\n",
            "Unpacking libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Setting up libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Name: tensorflow\n",
            "Version: 2.3.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: keras-preprocessing, tensorboard, google-pasta, termcolor, numpy, wrapt, wheel, grpcio, scipy, protobuf, six, tensorflow-estimator, absl-py, opt-einsum, h5py, astunparse, gast\n",
            "Required-by: fancyimpute\n",
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.31.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.35.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (49.6.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Collecting tensorflow==1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/55/7995cc1e9e60fa37ea90e6777d832e75026fde5c6109215d892aaff2e9b7/tensorflow-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (35.0MB)\n",
            "\u001b[K     |████████████████████████████████| 35.0MB 90kB/s \n",
            "\u001b[?25hCollecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 51.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (0.35.1)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (1.15.0)\n",
            "Collecting backports.weakref==1.0rc1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/f7/ae34b6818b603e264f26fe7db2bd07850ce331ce2fde74b266d61f4a2d87/backports.weakref-1.0rc1-py3-none-any.whl\n",
            "Collecting markdown==2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/99/288a81a38526a42c98b5b9832c6e339ca8d5dd38b19a53abfac7c8037c7f/Markdown-2.2.0.tar.gz (236kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorflow==1.2) (49.6.0)\n",
            "Building wheels for collected packages: html5lib, markdown\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=7be609bf6447cb1455b51f2f3d2885948df6b0f5d3fb4a5c4658a5be796e5151\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "  Building wheel for markdown (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for markdown: filename=Markdown-2.2.0-cp36-none-any.whl size=136272 sha256=06eddc66ece73d2df8eefd61ffb9edb33f1f15b34c212cfb0fef42f200b0525b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/52/17/f0af18e3e0ec6fa60b361ffed15b4c3468f6f3bcdb87fbe079\n",
            "Successfully built html5lib markdown\n",
            "\u001b[31mERROR: tensorboard 2.3.0 has requirement markdown>=2.6.8, but you'll have markdown 2.2.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: html5lib, bleach, backports.weakref, markdown, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.5\n",
            "    Uninstalling bleach-3.1.5:\n",
            "      Successfully uninstalled bleach-3.1.5\n",
            "  Found existing installation: Markdown 3.2.2\n",
            "    Uninstalling Markdown-3.2.2:\n",
            "      Successfully uninstalled Markdown-3.2.2\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed backports.weakref-1.0rc1 bleach-1.5.0 html5lib-0.9999999 markdown-2.2.0 tensorflow-1.2.0\n",
            "Collecting keras==2.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/45/f69527dca07582dddce1e66921484e50802f4d15095a72c793e15723bf46/Keras-2.0.6.tar.gz (228kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: theano in /usr/local/lib/python3.6/dist-packages (from keras==2.0.6) (1.0.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.0.6) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==2.0.6) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from theano->keras==2.0.6) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from theano->keras==2.0.6) (1.4.1)\n",
            "Building wheels for collected packages: keras\n",
            "  Building wheel for keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras: filename=Keras-2.0.6-cp36-none-any.whl size=266419 sha256=685dea8bb0d73c4248ab025d64cf317e8b1d8cc9c7fe64dcb521dcbfac7ba773\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/70/83/9be5aef9c4c863ea21adacd0be83139b20d3d819401a2b07d3\n",
            "Successfully built keras\n",
            "\u001b[31mERROR: textgenrnn 1.4.1 has requirement keras>=2.1.5, but you'll have keras 2.0.6 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.0.6\n",
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/f5/0ad780f454595e641bc89b6a37b6a5f9be86146a404609ff47d33e2b1026/tf_nightly-2.4.0.dev20200829-cp36-cp36m-manylinux2010_x86_64.whl (344.7MB)\n",
            "\u001b[K     |████████████████████████████████| 344.7MB 48kB/s \n",
            "\u001b[?25hCollecting flatbuffers>=1.12\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.31.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.35.1)\n",
            "Collecting tb-nightly<3.0.0a0,>=2.4.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/c8/0a028f3d76237c2337fe5cddc639c6fad5fbfaca64db15284ab5da83e781/tb_nightly-2.4.0a20200829-py3-none-any.whl (9.0MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0MB 55.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.7.4.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.8.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.18.5)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Collecting tf-estimator-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/7a/d31566b05f2b0a6c4713eb8c3c98fe967de651b075627d5cd30582a6042c/tf_estimator_nightly-2.4.0.dev2020082901-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 49.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tf-nightly) (49.6.0)\n",
            "Collecting markdown>=2.6.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/63/eaec2bd025ab48c754b55e8819af0f6a69e2b1e187611dd40cbbe101ee7f/Markdown-3.2.2-py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.17.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.4.8)\n",
            "\u001b[31mERROR: tensorflow 1.2.0 has requirement markdown==2.2.0, but you'll have markdown 3.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: flatbuffers, markdown, tb-nightly, tf-estimator-nightly, tf-nightly\n",
            "  Found existing installation: Markdown 2.2.0\n",
            "    Uninstalling Markdown-2.2.0:\n",
            "      Successfully uninstalled Markdown-2.2.0\n",
            "Successfully installed flatbuffers-1.12 markdown-3.2.2 tb-nightly-2.4.0a20200829 tf-estimator-nightly-2.4.0.dev2020082901 tf-nightly-2.4.0.dev20200829\n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 144584 files and directories currently installed.)\n",
            "Preparing to unpack .../libarchive-dev_3.2.2-3.1ubuntu0.6_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.2.2-3.1ubuntu0.6) ...\n",
            "Setting up libarchive-dev:amd64 (3.2.2-3.1ubuntu0.6) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting libarchive\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/d4/26f5c9835d4d648e4f22b5fb91288457698e928aaf9d4ab7eff405b7ef03/libarchive-0.4.7.tar.gz\n",
            "Collecting nose\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 6.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: libarchive\n",
            "  Building wheel for libarchive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libarchive: filename=libarchive-0.4.7-cp36-none-any.whl size=31632 sha256=a563650bf915d17e8809c3ce38742c402f7d3c86d8ae4e7c1a184891703c0c1e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/5c/fa/92ee330d259e8fa5bedbd53f67040710fe81cfa463b8711d26\n",
            "Successfully built libarchive\n",
            "Installing collected packages: nose, libarchive\n",
            "Successfully installed libarchive-0.4.7 nose-1.3.7\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "(Reading database ... 144640 files and directories currently installed.)\n",
            "Preparing to unpack .../0-python-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python-pyshp.\n",
            "Preparing to unpack .../1-python-pyshp_1.2.12+ds-1_all.deb ...\n",
            "Unpacking python-pyshp (1.2.12+ds-1) ...\n",
            "Selecting previously unselected package python-shapely.\n",
            "Preparing to unpack .../2-python-shapely_1.6.4-1_amd64.deb ...\n",
            "Unpacking python-shapely (1.6.4-1) ...\n",
            "Selecting previously unselected package python-six.\n",
            "Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n",
            "Unpacking python-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python-cartopy:amd64.\n",
            "Preparing to unpack .../4-python-cartopy_0.14.2+dfsg1-2build3_amd64.deb ...\n",
            "Unpacking python-cartopy:amd64 (0.14.2+dfsg1-2build3) ...\n",
            "Selecting previously unselected package python3-pkg-resources.\n",
            "Preparing to unpack .../5-python3-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python3-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python3-pyshp.\n",
            "Preparing to unpack .../6-python3-pyshp_1.2.12+ds-1_all.deb ...\n",
            "Unpacking python3-pyshp (1.2.12+ds-1) ...\n",
            "Selecting previously unselected package python3-shapely.\n",
            "Preparing to unpack .../7-python3-shapely_1.6.4-1_amd64.deb ...\n",
            "Unpacking python3-shapely (1.6.4-1) ...\n",
            "Selecting previously unselected package python3-six.\n",
            "Preparing to unpack .../8-python3-six_1.11.0-2_all.deb ...\n",
            "Unpacking python3-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python3-cartopy:amd64.\n",
            "Preparing to unpack .../9-python3-cartopy_0.14.2+dfsg1-2build3_amd64.deb ...\n",
            "Unpacking python3-cartopy:amd64 (0.14.2+dfsg1-2build3) ...\n",
            "Setting up python-shapely (1.6.4-1) ...\n",
            "Setting up python-pyshp (1.2.12+ds-1) ...\n",
            "Setting up python3-six (1.11.0-2) ...\n",
            "Setting up python3-shapely (1.6.4-1) ...\n",
            "Setting up python3-pyshp (1.2.12+ds-1) ...\n",
            "Setting up python3-pkg-resources (39.0.1-2) ...\n",
            "Setting up python-pkg-resources (39.0.1-2) ...\n",
            "Setting up python-six (1.11.0-2) ...\n",
            "Setting up python3-cartopy:amd64 (0.14.2+dfsg1-2build3) ...\n",
            "Setting up python-cartopy:amd64 (0.14.2+dfsg1-2build3) ...\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWRVWDpXmtyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Me-yKcKR_6v",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "! shred -u setup_google_colab.py\n",
        "! wget https://raw.githubusercontent.com/hse-aml/intro-to-dl/master/setup_google_colab.py -O setup_google_colab.py\n",
        "import setup_google_colab\n",
        "# please, uncomment the week you're working on\n",
        "# setup_google_colab.setup_week1()\n",
        "# setup_google_colab.setup_week2()\n",
        "setup_google_colab.setup_week3()\n",
        "# setup_google_colab.setup_week4()\n",
        "# setup_google_colab.setup_week5()\n",
        "# setup_google_colab.setup_week6()\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "import grading\n",
        "import download_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HdSyOdpwR_69",
        "colab": {}
      },
      "source": [
        "# !!! remember to clear session/graph if you rebuild your graph to avoid out-of-memory errors !!!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5sE6zPIkR_7G",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "download_utils.link_all_keras_resources()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-03T13:00:39.210959Z",
          "start_time": "2017-09-03T13:00:39.057800Z"
        },
        "colab_type": "code",
        "id": "oFsfe-Y3R_7U",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "!pip install keras\n",
        "import keras \n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "print(tf.__version__)\n",
        "print(keras.__version__)\n",
        "import cv2  # for image processing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.io\n",
        "import os\n",
        "import tarfile\n",
        "import keras_utils\n",
        "from keras_utils import reset_tf_session "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ck-5zONJR_7c"
      },
      "source": [
        "# Fill in your Coursera token and email\n",
        "To successfully submit your answers to our grader, please fill in your Coursera submission token and email"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "25xCAOIXR_7l",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "grader = grading.Grader(assignment_key=\"2v-uxpD7EeeMxQ6FWsz5LA\", \n",
        "                        all_parts=[\"wuwwC\", \"a4FK1\", \"qRsZ1\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sOqo6jM8R_7s",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# token expires every 30 min\n",
        "#COURSERA_TOKEN ='PdTQTWlLbtye9aYC' ### YOUR TOKEN HERE\n",
        "#\n",
        "COURSERA_EMAIL ='sahelirima23@gmail.com' ### YOUR EMAIL HER"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fvEuuQjXR_72"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P0hnp_xeR_77"
      },
      "source": [
        "Dataset was downloaded for you, it takes 12 min and 400mb.\n",
        "Relevant links (just in case):\n",
        "- http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html\n",
        "- http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
        "- http://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z5cR0FPfR_8C",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# we downloaded them for you, just link them here\n",
        "download_utils.link_week_3_resources()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SstspCFJR_8M"
      },
      "source": [
        "# Prepare images for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-03T13:00:39.214524Z",
          "start_time": "2017-09-03T13:00:39.212453Z"
        },
        "colab_type": "code",
        "id": "hEn0M12cR_8P",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# we will crop and resize input images to IMG_SIZE x IMG_SIZE\n",
        "IMG_SIZE = 250"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-10T12:46:09.790818Z",
          "start_time": "2017-09-10T12:46:09.777771Z"
        },
        "colab_type": "code",
        "id": "OU03A3j1R_8X",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "def decode_image_from_raw_bytes(raw_bytes):\n",
        "    img = cv2.imdecode(np.asarray(bytearray(raw_bytes), dtype=np.uint8), 1)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v8WrFHKOR_8h"
      },
      "source": [
        "We will take a center crop from each image like this:\n",
        "<img src=\"https://github.com/hse-aml/intro-to-dl/blob/master/week3/images/center_crop.jpg?raw=1\" style=\"width:50%\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-03T13:00:39.393675Z",
          "start_time": "2017-09-03T13:00:39.302130Z"
        },
        "colab_type": "code",
        "id": "N3XYze6NR_8i",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "def image_center_crop(img):\n",
        "    \"\"\"\n",
        "    Makes a square center crop of an img, which is a [h, w, 3] numpy array.\n",
        "    Returns [min(h, w), min(h, w), 3] output with same width and height.\n",
        "    For cropping use numpy slicing.\n",
        "    \"\"\"\n",
        "    \n",
        "    h, w = img.shape[0], img.shape[1]\n",
        "    m = min(h, w)\n",
        "    cropped_img = img[(h-m)//2:(h+m)//2, (w-m)//2:(w+m)//2, :]\n",
        "    \n",
        "    return cropped_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-03T13:00:39.506473Z",
          "start_time": "2017-09-03T13:00:39.395470Z"
        },
        "colab_type": "code",
        "id": "iifgxQhMR_8r",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "def prepare_raw_bytes_for_model(raw_bytes, normalize_for_model=True):\n",
        "    img = decode_image_from_raw_bytes(raw_bytes)  # decode image raw bytes to matrix\n",
        "    img = image_center_crop(img)  # take squared center crop\n",
        "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))  # resize for our model\n",
        "    if normalize_for_model:\n",
        "        img = img.astype(\"float32\")  # prepare for normalization\n",
        "        img = keras.applications.inception_v3.preprocess_input(img)  # normalize for model\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RkJgEjvVR_86",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# reads bytes directly from tar by filename (slow, but ok for testing, takes ~6 sec)\n",
        "def read_raw_from_tar(tar_fn, fn):\n",
        "    with tarfile.open(tar_fn) as f:\n",
        "        m = f.getmember(fn)\n",
        "        return f.extractfile(m).read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-03T13:00:39.961301Z",
          "start_time": "2017-09-03T13:00:39.508004Z"
        },
        "colab_type": "code",
        "id": "RgMdNKNnR_9B",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# test cropping\n",
        "raw_bytes = read_raw_from_tar(\"102flowers.tgz\", \"jpg/image_00001.jpg\")\n",
        "\n",
        "img = decode_image_from_raw_bytes(raw_bytes)\n",
        "print(img.shape)\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "img = prepare_raw_bytes_for_model(raw_bytes, normalize_for_model=False)\n",
        "print(img.shape)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DArjXOGAR_9G",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "## GRADED PART, DO NOT CHANGE!\n",
        "# Test image preparation for model\n",
        "prepared_img = prepare_raw_bytes_for_model(read_raw_from_tar(\"102flowers.tgz\", \"jpg/image_00001.jpg\"))\n",
        "grader.set_answer(\"qRsZ1\", list(prepared_img.shape) + [np.mean(prepared_img), np.std(prepared_img)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "spb27xklR_9f",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# you can make submission with answers so far to check yourself at this stage\n",
        "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F_b6hSULR_9k"
      },
      "source": [
        "# Prepare for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DmeaGxPZR_9m",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# read all filenames and labels for them\n",
        "\n",
        "# read filenames firectly from tar\n",
        "def get_all_filenames(tar_fn):\n",
        "    with tarfile.open(tar_fn) as f:\n",
        "        return [m.name for m in f.getmembers() if m.isfile()]\n",
        "\n",
        "# list all files in tar sorted by name\n",
        "all_files = sorted(get_all_filenames(\"102flowers.tgz\")) \n",
        "# read class labels (0, 1, 2, ...)\n",
        "all_labels = scipy.io.loadmat('imagelabels.mat')['labels'][0] - 1  \n",
        "\n",
        "# all_files and all_labels are aligned now\n",
        "N_CLASSES = len(np.unique(all_labels))\n",
        "print(N_CLASSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-03T13:00:40.185940Z",
          "start_time": "2017-09-03T13:00:40.175758Z"
        },
        "colab_type": "code",
        "id": "WeMMtfGLR_9y",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# split into train/test\n",
        "tr_files, te_files, tr_labels, te_labels = \\\n",
        "    train_test_split(all_files, all_labels, test_size=0.2, random_state=42, stratify=all_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-mfpaSQgR_-X",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# will yield raw image bytes from tar with corresponding label\n",
        "def raw_generator_with_label_from_tar(tar_fn, files, labels):\n",
        "    label_by_fn = dict(zip(files, labels))\n",
        "    with tarfile.open(tar_fn) as f:\n",
        "        while True:\n",
        "            m = f.next()\n",
        "            if m is None:\n",
        "                break\n",
        "            if m.name in label_by_fn:\n",
        "                yield f.extractfile(m).read(), label_by_fn[m.name]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-03T13:00:40.529088Z",
          "start_time": "2017-09-03T13:00:40.423114Z"
        },
        "colab_type": "code",
        "id": "ptrZsGHmR_-t",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# batch generator\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def batch_generator(items, batch_size):\n",
        "    \"\"\"\n",
        "    Implement batch generator that yields items in batches of size batch_size.\n",
        "    There's no need to shuffle input items, just chop them into batches.\n",
        "    Remember about the last batch that can be smaller than batch_size!\n",
        "    Input: any iterable (list, generator, ...). You should do `for item in items: ...`\n",
        "        In case of generator you can pass through your items only once!\n",
        "    Output: In output yield each batch as a list of items.\n",
        "    \"\"\"\n",
        "    \n",
        "    ###\n",
        "    batch = []\n",
        "    for i, item in enumerate(items):\n",
        "        batch.append(item)\n",
        "        if i % batch_size == batch_size-1:\n",
        "            yield batch\n",
        "            batch = []\n",
        "    # return last (incomplete) batch\n",
        "    if batch[0]:\n",
        "        yield [item for item in batch if item]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dy9TDdGIR_--",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "## GRADED PART, DO NOT CHANGE!\n",
        "# Test batch generator\n",
        "def _test_items_generator():\n",
        "    for i in range(10):\n",
        "        yield i\n",
        "\n",
        "grader.set_answer(\"a4FK1\", list(map(lambda x: len(x), batch_generator(_test_items_generator(), 3))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Okt2zOatR__K",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# you can make submission with answers so far to check yourself at this stage\n",
        "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-03T13:00:40.637615Z",
          "start_time": "2017-09-03T13:00:40.530642Z"
        },
        "colab_type": "code",
        "id": "g5fmQQBBR__V",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "def train_generator(files, labels):\n",
        "    while True:  # so that Keras can loop through this as long as it wants\n",
        "        for batch in batch_generator(raw_generator_with_label_from_tar(\n",
        "                \"102flowers.tgz\", files, labels), BATCH_SIZE):\n",
        "            # prepare batch images\n",
        "            batch_imgs = []\n",
        "            batch_targets = []\n",
        "            for raw, label in batch:\n",
        "                img = prepare_raw_bytes_for_model(raw)\n",
        "                batch_imgs.append(img)\n",
        "                batch_targets.append(label)\n",
        "            # stack images into 4D tensor [batch_size, img_size, img_size, 3]\n",
        "            batch_imgs = np.stack(batch_imgs, axis=0)\n",
        "            # convert targets into 2D tensor [batch_size, num_classes]\n",
        "            batch_targets = keras.utils.np_utils.to_categorical(batch_targets, N_CLASSES)\n",
        "            yield batch_imgs, batch_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-03T13:00:41.092659Z",
          "start_time": "2017-09-03T13:00:40.639132Z"
        },
        "colab_type": "code",
        "id": "GyihiuCER__l",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# test training generator\n",
        "for _ in train_generator(tr_files, tr_labels):\n",
        "    print(_[0].shape, _[1].shape)\n",
        "    plt.imshow(np.clip(_[0][0] / 2. + 0.5, 0, 1))\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MWV-h4UFR__p"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-10T13:16:35.109044Z",
          "start_time": "2017-09-10T13:16:35.105127Z"
        },
        "colab_type": "text",
        "id": "0i50bdHZR__r"
      },
      "source": [
        "You cannot train such a huge architecture from scratch with such a small dataset.\n",
        "\n",
        "But using fine-tuning of last layers of pre-trained network you can get a pretty good classifier very quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-03T13:00:41.097588Z",
          "start_time": "2017-09-03T13:00:41.094216Z"
        },
        "colab_type": "code",
        "id": "SqjuZzycR__s",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# remember to clear session if you start building graph from scratch!\n",
        "s = reset_tf_session()\n",
        "# don't call K.set_learning_phase() !!! (otherwise will enable dropout in train/test simultaneously)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-03T13:00:41.222209Z",
          "start_time": "2017-09-03T13:00:41.098974Z"
        },
        "colab_type": "code",
        "id": "KZKPVNadR__w",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "def inception(use_imagenet=True):\n",
        "    # load pre-trained model graph, don't add final layer\n",
        "    model = keras.applications.InceptionV3(include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "                                          weights='imagenet' if use_imagenet else None)\n",
        "    # add global pooling just like in InceptionV3\n",
        "    new_output = keras.layers.GlobalAveragePooling2D()(model.output)\n",
        "    # add new dense layer for our labels\n",
        "    new_output = keras.layers.Dense(N_CLASSES, activation='softmax')(new_output)\n",
        "    model = keras.engine.training.Model(model.inputs, new_output)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-03T13:00:45.150429Z",
          "start_time": "2017-09-03T13:00:41.223777Z"
        },
        "colab_type": "code",
        "id": "4vdhas9XR__3",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "model = inception()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-03T13:00:45.252883Z",
          "start_time": "2017-09-03T13:00:45.152062Z"
        },
        "colab_type": "code",
        "id": "j_H_36S6R__9",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-03T13:00:45.273005Z",
          "start_time": "2017-09-03T13:00:45.254250Z"
        },
        "colab_type": "code",
        "id": "M5n16WPBSAAB",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# how many layers our model has\n",
        "print(len(model.layers))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-03T13:00:45.370171Z",
          "start_time": "2017-09-03T13:00:45.274354Z"
        },
        "colab_type": "code",
        "id": "xby5w8OBSAAI",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# set all layers trainable by default\n",
        "for layer in model.layers:\n",
        "    layer.trainable = True\n",
        "    if isinstance(layer, keras.layers.BatchNormalization):\n",
        "        # we do aggressive exponential smoothing of batch norm\n",
        "        # parameters to faster adjust to our new dataset\n",
        "        layer.momentum = 0.9\n",
        "    \n",
        "# fix deep layers (fine-tuning only last 50)\n",
        "for layer in model.layers[:-50]:\n",
        "    # fix all but batch norm layers, because we neeed to update moving averages for a new dataset!\n",
        "    if not isinstance(layer, keras.layers.BatchNormalization):\n",
        "        layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-03T13:00:45.494833Z",
          "start_time": "2017-09-03T13:00:45.371512Z"
        },
        "colab_type": "code",
        "id": "eqh13ceoSAAM",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# compile new model\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',  # we train 102-way classification\n",
        "    optimizer=keras.optimizers.adamax(lr=1e-2),  # we can take big lr here because we fixed first layers\n",
        "    metrics=['accuracy']  # report accuracy during training\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Q-wK1OnSAAT",
        "colab": {}
      },
      "source": [
        "# we will save model checkpoints to continue training in case of kernel death\n",
        "model_filename = 'flowers.{0:03d}.hdf5'\n",
        "last_finished_epoch = None\n",
        "\n",
        "#### uncomment below to continue training from model checkpoint\n",
        "#### fill `last_finished_epoch` with your latest finished epoch\n",
        "# \n",
        "from keras.models import load_model\n",
        "# \n",
        "s = reset_tf_session()\n",
        "# \n",
        "last_finished_epoch = 15\n",
        "# \n",
        "model = load_model(model_filename.format(last_finished_epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cq8Vm_dRSAAy"
      },
      "source": [
        "Training takes **2 hours**. You're aiming for ~0.93 validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-09-03T14:23:36.792701Z",
          "start_time": "2017-09-03T13:00:45.496194Z"
        },
        "colab_type": "code",
        "id": "FLoAoxwxSAA0",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# fine tune for 2 epochs (full passes through all training data)\n",
        "# we make 2*8 epochs, where epoch is 1/8 of our training data to see progress more often\n",
        "model.fit_generator(\n",
        "    train_generator(tr_files, tr_labels), \n",
        "    steps_per_epoch=len(tr_files) // BATCH_SIZE // 8,\n",
        "    epochs=2 * 8,\n",
        "    validation_data=train_generator(te_files, te_labels), \n",
        "    validation_steps=len(te_files) // BATCH_SIZE // 4,\n",
        "    callbacks=[keras_utils.TqdmProgressCallback(), \n",
        "               keras_utils.ModelSaveCallback(model_filename)],\n",
        "    verbose=0,\n",
        "    initial_epoch=last_finished_epoch or 15\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H1NgCQFUSABE",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "## GRADED PART, DO NOT CHANGE!\n",
        "# Accuracy on validation set\n",
        "test_accuracy = model.evaluate_generator(\n",
        "    train_generator(te_files, te_labels), \n",
        "    len(te_files) // BATCH_SIZE // 2\n",
        ")\n",
        "#[1]\n",
        "grader.set_answer(\"wuwwC\", test_accuracy)\n",
        "print(test_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDK6VYE1ygPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grader = grading.Grader(assignment_key=\"2v-uxpD7EeeMxQ6FWsz5LA\", \n",
        "                        all_parts=[\"wuwwC\", \"a4FK1\", \"qRsZ1\"])\n",
        "# token expires every 30 min\n",
        "COURSERA_TOKEN ='PdTQTWlLbtye9aYC' ### YOUR TOKEN HERE\n",
        "COURSERA_EMAIL ='sahelirima23@gmail.com' ### YOUR EMAIL HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aLCYRZyWSABT",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# you can make submission with answers so far to check yourself at this stage\n",
        "\n",
        "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QM7uVA7dSABY"
      },
      "source": [
        "That's it! Congratulations!\n",
        "\n",
        "What you've done:\n",
        "- prepared images for the model\n",
        "- implemented your own batch generator\n",
        "- fine-tuned the pre-trained model"
      ]
    }
  ]
}